{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eabea3a9-72a7-488d-9be5-4810acc16438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Libraries management\n",
    "from pyspark import pipelines as pl\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "volume_path=\"/Volumes/workspace/damg7370/datastore/schema_drift/customer_*.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e57d5802-32d6-4b73-a116-ea56810c33cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#bronze layer table: cust_bronze_sd\n",
    "pl.create_streaming_table(\"demo_cust_bronze_sd\")\n",
    "\n",
    "# Ingest the raw data into the bronze table using append flow\n",
    "@pl.append_flow(\n",
    "  target = \"demo_cust_bronze_sd\", #object name\n",
    "  name = \"demo_cust_bronze_sd_ingest_flow\" #flow name\n",
    ")\n",
    "def demo_cust_bronze_sd_ingest_flow():\n",
    "  df = (\n",
    "      spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"json\")\n",
    "          .option(\"cloudFiles.inferColumnTypes\", \"true\") #auto scan schema \n",
    "          #.option(\"cloudFiles.schemaEvolutionMode\", \"failOnNewColumns\") # schema customer_data_1.json is different than customer_data_2.json so it fails with  [UNKNOWN_FIELD_EXCEPTION.NEW_FIELDS_IN_RECORD_WITH_FILE_PATH] excetion and stops processing\n",
    "          .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "          .load(f\"{volume_path}\")\n",
    "  )\n",
    "  return df.withColumn(\"ingestion_datetime\", current_timestamp())\\\n",
    "           .withColumn(\"source_filename\", col(\"_metadata.file_path\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "832857dd-cd36-4809-909b-64083171f595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "pl.create_streaming_table(\n",
    "   name = \"demo_cust_silver_sd\",\n",
    "   expect_all_or_drop = {\"no_rescued_data\": \"_rescued_data IS NULL\",\"valid_id\": \"CustomerID IS NOT NULL\"}\n",
    "   )\n",
    "@pl.append_flow(\n",
    "   target = \"demo_cust_silver_sd\",\n",
    "   name = \"demo_cust_silver_sd_clean_flow\"\n",
    " )\n",
    "def demo_cust_silver_sd_clean_flow():\n",
    "   return (\n",
    "       spark.readStream.table(\"demo_cust_bronze_sd\")\n",
    "   )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d759ccab-bd0e-40f4-9f78-bcab21b90192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def process__rescue_data_datatype_change(df, target_schema: StructType):\n",
    "    #Parse the _rescued_data json to a MAP (Key,Value) type and store in _rescued_data_modified column\n",
    "    df = df.withColumn(\"_rescued_data_modified\", from_json(col(\"_rescued_data\"), MapType(StringType(), StringType())))\n",
    "    \n",
    "    for field in target_schema.fields:\n",
    "        data_type = field.dataType\n",
    "        column_name = field.name\n",
    "\n",
    "        # Check if \"_rescue_data\" is not null and if the key exists\n",
    "        # pyspark.sql.functions.map_contains_key function in PySpark is used to check if a specified key exists within a MapType column in a DataFrame. returns T/F\n",
    "        key_condition = expr(f\"_rescued_data_modified IS NOT NULL AND map_contains_key(_rescued_data_modified, '{column_name}')\")\n",
    "        \n",
    "        # Extract the rescued value for this column, if it exists, and cast it to the target data type\n",
    "        rescued_value = when(key_condition, col(\"_rescued_data_modified\").getItem(column_name).cast(data_type)).otherwise(col(column_name).cast(data_type))\n",
    "        \n",
    "        # Update the DataFrame with the merged column\n",
    "        df = df.withColumn(column_name, rescued_value)\n",
    "        df = df.withColumn(column_name, col(column_name).cast(data_type))\n",
    "        \n",
    "    df = df.drop('_rescued_data_modified')\n",
    "\n",
    "    # Setting the _rescued_data to null after processing since we use the column to check qualit expectation for schema update\n",
    "    df = df.withColumn('_rescued_data', lit(None).cast(StringType()))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a67a0444-99f9-4031-b403-874ffef78bce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to handle adding NEW FIELDS \n",
    "def process__rescue_data_new_fields(df):\n",
    "\n",
    "    #Add all fields from _rescued_data to key map\n",
    "    df = df.withColumn(\n",
    "        \"_rescued_data_json_to_map\", \n",
    "        from_json(\n",
    "            col(\"_rescued_data\"), \n",
    "            MapType(StringType(), StringType())\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Extract all keys from _rescued_data_map_keys\n",
    "    df = df.withColumn(\"_rescued_data_map_keys\", map_keys(col(\"_rescued_data_json_to_map\")))\n",
    "\n",
    "    # Get all keys in all rows as a new DataFrame\n",
    "    df_keys = df.select(\n",
    "        explode(\n",
    "            map_keys(col(\"_rescued_data_json_to_map\"))\n",
    "        ).alias(\"rescued_key\")\n",
    "    ).distinct()\n",
    "\n",
    "    # Collect keys as a list (only if df is not streaming)\n",
    "    # If streaming, you must provide the list of possible keys another way\n",
    "    new_keys = [row[\"rescued_key\"] for row in df_keys.collect()] if not df.isStreaming else []\n",
    "\n",
    "    # Add new columns for each key\n",
    "    for key in new_keys:\n",
    "        if key != \"_file_path\":\n",
    "            df = df.withColumn(\n",
    "                key,\n",
    "                col(\"_rescued_data_json_to_map\").getItem(key).cast(StringType())\n",
    "            )\n",
    "\n",
    "    #***Ehnancement can be done by adding additional logic \n",
    "    #***  to exclude columns that are already in dataframe(Substract those columns)\n",
    "    #***  to infer datatype for new columns and use infered datatype instead of static stringtype\n",
    "    #***  additionally check if each column exists and dataframe has rows on each transformation and raise exception before using it\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4632de72-c6ea-477d-8ce9-a5b3e854b34f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "updated_datatypes = StructType([\n",
    "  # define the column signuoDate as DATE type and also make it nullable (Make 3rd argument False if you want to make it non nullable)\n",
    "  StructField(\"signupDate\", DateType(), True) \n",
    "])\n",
    "\n",
    "pl.create_streaming_table(\n",
    "  name = \"demo_cust_silver_sd\",\n",
    "  expect_all_or_drop = {\"no_rescued_data\": \"_rescued_data IS NULL\",\"valid_id\": \"CustomerID IS NOT NULL\"}\n",
    ")\n",
    "\n",
    "@pl.append_flow(\n",
    "  target = \"demo_cust_silver_sd\",\n",
    "  name = \"demo_cust_silver_sd_clean_flow\"\n",
    ")\n",
    "def demo_cust_silver_sd_clean_flow():\n",
    "  df = (\n",
    "    spark.readStream.table(\"demo_cust_bronze_sd\")\n",
    "  )\n",
    "  df = process__rescue_data_new_fields(df)\n",
    "  df = process__rescue_data_datatype_change(df, updated_datatypes)\n",
    "  return df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DemoSchemaDrift_Rescue",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
